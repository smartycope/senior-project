[
  {
    "objectID": "posts/SquarePacking/index.html",
    "href": "posts/SquarePacking/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/SquarePacking/Attempt2.html",
    "href": "posts/SquarePacking/Attempt2.html",
    "title": "senior project",
    "section": "",
    "text": "import gymnasium as gym\n# from gym.wrappers import FlattenObservation\n# from gym.utils.env_checker import check_env\n# import tabletop\n# import gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as ks\n# import gym\nimport matplotlib.pyplot as plt\nimport time\nfrom tensorflow.keras import layers\n# from gym.utils.play import play\n# from Cope import debug\n%load_ext autoreload\n%autoreload 2\nimport SquarePacking.env\n!pip install -e SquarePacking\nfrom ReplayBuffer import ReplayBuffer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nDefaulting to user installation because normal site-packages is not writeable\nObtaining file:///home/leonard/hello/python/AI/SquarePacking/SquarePacking\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: gymnasium in /home/leonard/.local/lib/python3.10/site-packages (from SquarePacking==0.0.1) (0.27.1)\nRequirement already satisfied: matplotlib in /home/leonard/.local/lib/python3.10/site-packages (from SquarePacking==0.0.1) (3.5.1)\nRequirement already satisfied: typing-extensions&gt;=4.3.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium-&gt;SquarePacking==0.0.1) (4.5.0)\nRequirement already satisfied: cloudpickle&gt;=1.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium-&gt;SquarePacking==0.0.1) (2.1.0)\nRequirement already satisfied: numpy&gt;=1.21.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium-&gt;SquarePacking==0.0.1) (1.21.6)\nRequirement already satisfied: gymnasium-notices&gt;=0.0.1 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium-&gt;SquarePacking==0.0.1) (0.0.1)\nRequirement already satisfied: jax-jumpy&gt;=0.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium-&gt;SquarePacking==0.0.1) (0.2.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (21.3)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (1.4.0)\nRequirement already satisfied: cycler&gt;=0.10 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (0.11.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (9.0.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (4.31.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (2.8.1)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib-&gt;SquarePacking==0.0.1) (3.0.7)\nRequirement already satisfied: six&gt;=1.5 in /usr/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;SquarePacking==0.0.1) (1.16.0)\nInstalling collected packages: SquarePacking\n  Attempting uninstall: SquarePacking\n    Found existing installation: SquarePacking 0.0.1\n    Uninstalling SquarePacking-0.0.1:\n      Successfully uninstalled SquarePacking-0.0.1\n  Running setup.py develop for SquarePacking\nSuccessfully installed SquarePacking\n\n\n\n# Actor-Critic Model from Online\n\n# The inputs are in the obvervation space\nnum_inputs = 3*N\n# We want to nudge each box a little in each parameter\nnum_outputs = num_inputs\n\ninputs = layers.Input(shape=(num_inputs,))\ncommon = layers.Dense(128, activation=\"relu\")(inputs)\naction = layers.Dense(num_outputs, activation=\"tanh\")(common)\ncritic = layers.Dense(1)(common)\n\nmodel = keras.Model(inputs=inputs, outputs=[action, critic])\n\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nhuber_loss = keras.losses.Huber()\naction_probs_history = []\ncritic_value_history = []\nrewards_history = []\nrunning_reward = 0\nepisode_count = 0\n\nwhile True:  # Run until solved\n    # obs, info = env.reset(seed=seed)\n    obs, info = env.reset()\n    episode_reward = 0\n\n    with tf.GradientTape() as tape:\n        for timestep in range(1, max_steps_per_episode):\n            if GUI:\n                env.render()\n\n            # state = tf.convert_to_tensor(np.reshape(np.array((state['ant'], state['nearest food'], state['has food'])), (3,)))\n            state = tf.convert_to_tensor(obs)\n            state = tf.expand_dims(state, 0)\n\n            # Predict action probabilities and estimated future rewards\n            # from environment state\n            action_probs, critic_value = model(state)\n            critic_value_history.append(critic_value[0, 0])\n\n            # Sample action from action probability distribution\n            # I don't think this works, because \n            # action = np.random.choice(num_outputs, p=np.squeeze(action_probs))\n            # action_probs_history.append(tf.math.log(action_probs[0, action]))\n            action_probs_history.append(tf.math.log(action_probs))\n\n            # Apply the sampled action in our environment\n            state, reward, done, info = env.step(action_probs)\n            rewards_history.append(reward)\n            episode_reward += reward\n\n            if done:\n                break\n\n        # Update running reward to check condition for solving\n        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n        # Calculate expected value from rewards\n        # - At each timestep what was the total reward received after that timestep\n        # - Rewards in the past are discounted by multiplying them with gamma\n        # - These are the labels for our critic\n        returns = []\n        discounted_sum = 0\n        for r in rewards_history[::-1]:\n            discounted_sum = r + patience * discounted_sum\n            returns.insert(0, discounted_sum)\n\n        # Normalize\n        returns = np.array(returns)\n        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n        returns = returns.tolist()\n\n        # Calculating loss values to update our network\n        history = zip(action_probs_history, critic_value_history, returns)\n        actor_losses = []\n        critic_losses = []\n        for log_prob, value, ret in history:\n            # At this point in history, the critic estimated that we would get a\n            # total reward = `value` in the future. We took an action with log probability\n            # of `log_prob` and ended up recieving a total reward = `ret`.\n            # The actor must be updated so that it predicts an action that leads to\n            # high rewards (compared to critic's estimate) with high probability.\n            diff = ret - value\n            actor_losses.append(-log_prob * diff)  # actor loss\n\n            # The critic must be updated so that it predicts a better estimate of\n            # the future rewards.\n            critic_losses.append(\n                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n            )\n\n        # Backpropagation\n        loss_value = sum(actor_losses) + sum(critic_losses)\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        # Clear the loss and reward history\n        action_probs_history.clear()\n        critic_value_history.clear()\n        rewards_history.clear()\n\n    # Log details\n    episode_count += 1\n    if episode_count % 10 == 0:\n        template = \"running reward: {:.2f} at episode {}\"\n        print(f\"running reward: {running_reward:.2f} at episode {episode_count}\")\n\n    # if running_reward &gt; 195:  # Condition to consider the task solved\n    if info['solved']:\n        print(f\"Solved at episode {episode_count}!\")\n        break\n\n\n# Configuration & Enviorment\n\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n# Set up the enviorment\nenv      = gym.make(\"SquarePacking/Square-v0\", N=4, render_mode=\"shapely\", shift_rate=.05, rot_rate=.05)\ntest_env = gym.make(\"SquarePacking/Square-v0\", N=4, render_mode=\"shapely\", shift_rate=.05, rot_rate=.05)\n\n# Discount factor for past rewards. (Always between 0 and 1.)\npatience = gamma = 0.99\n# Number of epochs to run and train agent.\nepochs = 100\n# Number of steps of interaction (state-action pairs) for the agent and the environment in each epoch.\nsteps_per_epoch = 4000\n# Maximum length of replay buffer.\nreplay_size = int(1e6)\n# Interpolation factor in polyak averaging for target networks. Target networks are updated towards main networks according to:\n# θ_target &lt;- pθ_target + (1-p)θ\n# where p is polyak. (Always between 0 and 1, usually close to 1.)\npolyak = 0.995\n# Learning rate for policy.\npi_lr = policy_lr = 1e-3\n# Learning rate for Q-networks.\nq_lr = Q_lr = 1e-3\n# Minibatch size for SGD.\nbatch_size = 100\n# Number of steps for uniform-random action selection, before running real policy. Helps exploration.\nstart_steps = 10000\n# Number of env interactions to collect before starting to do gradient descent updates. Ensures replay buffer is full enough for useful updates.\nupdate_after = 1000\n# Number of env interactions that should elapse between gradient descent updates. Note: Regardless of how long\n# you wait between updates, the ratio of env steps to gradient steps is locked to 1.\nupdate_every = 50\n# Stddev for Gaussian exploration noise added to policy at training time. (At test time, no noise is added.)\nact_noise = 0.1\n# Number of episodes to test the deterministic policy at the end of each epoch.\nnum_test_episodes = 10\n# Maximum length of trajectory / episode / rollout.\nmax_ep_len = 1000\n# The space dimentions\nobs_dim = env.observation_space.shape[0]\nact_dim = env.action_space.shape[0]\n# Action limit for clamping: critically, assumes all dimensions share the same bound!\nact_limit = env.action_space.high[0]\n# Inputs to computation graph\n# x_ph, a_ph, x2_ph, r_ph, d_ph = [tf.placeholder(dtype=tf.float32, shape=(None,dim) if dim else (None,)) for dim in (obs_dim, act_dim, obs_dim, None, None)]\n# x_ph, a_ph, x2_ph, r_ph, d_ph = obs_dim, act_dim, obs_dim, None, None\n# Experience buffer\nreplay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n\n# How often (in terms of gap between epochs) to save the current policy and value function.\n# save_freq = 1\n# Any kwargs appropriate for the actor_critic function you provided to DDPG.\n# ac_kwargs = dict(hidden_sizes=[args.hid]*args.l)\n# Share information about action space with policy architecture\n# ac_kwargs['action_space'] = env.action_space\n\n\n#? Don't know what this means\n# eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n\n/home/leonard/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: WARN: Box bound precision lowered by casting to float32\n  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n/home/leonard/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:35: UserWarning: WARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (4, 3)\n  logger.warn(\n\n\n\n# Main outputs from computation graph\n\n# Actor-critic functions\n\ndef actor_critic():\n    \"\"\" A function which takes in placeholder symbols for state, ``x_ph``, and action, ``a_ph``, \n    and returns the main outputs from the agent's Tensorflow computation graph:\n    ===========  ================  ======================================\n    Symbol       Shape             Description\n    ===========  ================  ======================================\n    ``pi``       (batch, act_dim)  | Deterministically computes actions\n                                   | from policy given states.\n    ``q``        (batch,)          | Gives the current estimate of Q* for\n                                   | states in ``x_ph`` and actions in\n                                   | ``a_ph``.\n    ``q_pi``     (batch,)          | Gives the composition of ``q`` and\n                                   | ``pi`` for states in ``x_ph``:\n                                   | q(x, pi(x)).\n    ===========  ================  ======================================\n    \"\"\"\n\n    def mlp(size, hidden_sizes=(256, 256)):\n        for h in hidden_sizes[:-1]:\n            unknownVar = tf.layers.Dense(size, units=h, activation='relu')\n        return tf.layers.Dense(unknownVar, units=hidden_sizes[-1], activation='tanh')\n\n    _act_dim = act_dim.shape.as_list()[-1]\n    _act_limit = env.action_space.high[0]\n\n    pi = act_limit * mlp(list(hidden_sizes)+[act_dim])\n    q = tf.squeeze(mlp(tf.concat([obs_dim, act_dim], axis=-1), list(hidden_sizes)+[1], tf.nn.relu, None), axis=1)\n    q_pi = tf.squeeze(mlp(tf.concat([obs_dim, pi], axis=-1), list(hidden_sizes)+[1], tf.nn.relu, None), axis=1)\n\n    return pi, q, q_pi\n\n\n# with tf.variable_scope('main'):\npi, q, q_pi = actor_critic(obs_dim, act_dim, **ac_kwargs)\n\n# Target networks\n# with tf.variable_scope('target'):\n# Note that the action placeholder going to actor_critic here is irrelevant, because we only need q_targ(s, pi_targ(s)).\n# ac_kwargs = dict(hidden_sizes=[args.hid]*args.l)\n# ac_kwargs['action_space'] = env.action_space\npi_targ, _, q_pi_targ  = actor_critic(obs_dim, act_dim, hidden_sizes=[args.hid]*args.l, action_space=)\n\n# Count variables\n# var_counts = tuple(core.count_vars(scope) for scope in ['main/pi', 'main/q', 'main'])\n# print('\\nNumber of parameters: \\t pi: %d, \\t q: %d, \\t total: %d\\n'%var_counts)\n\n\n# Bellman backup for Q function\n# backup = tf.stop_gradient(r_ph + patience*(1-d_ph)*q_pi_targ)\nbackup = tf.stop_gradient(r_ph + patience*(1-int(done))*q_pi_targ)\n\n# DDPG losses\npi_loss = -tf.reduce_mean(q_pi)\nq_loss = tf.reduce_mean((q-backup)**2)\n\n# Separate train ops for pi, q\npi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\nq_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\ntrain_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\ntrain_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n\n# Polyak averaging for target variables\ntarget_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n                            for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n\n# Initializing targets to match main variables\ntarget_init = tf.group([tf.assign(v_targ, v_main)\n                            for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(target_init)\n\n# Prepare for interaction with environment\ntotal_steps = steps_per_epoch * epochs\nstart_time = time.time()\no, ep_ret, ep_len = env.reset(), 0, 0\n\n\nNameError: name 'ac_kwargs' is not defined\n\n\n\n\ndef get_action(obs, noise_scale):\n    action = sess.run(pi, feed_dict={x_ph: obs.reshape(1,-1)})[0]\n    action += noise_scale * np.random.randn(act_dim)\n    return np.clip(action, -act_limit, act_limit)\n\ndef test_agent():\n    for j in range(num_test_episodes):\n        obs, done, ep_ret, ep_len = test_env.reset(), False, 0, 0\n        while not(done or (ep_len == max_ep_len)):\n            # Take deterministic actions at test time (noise_scale=0)\n            obs, reward, done, _ = test_env.step(get_action(obs, 0))\n            ep_ret += reward\n            ep_len += 1\n        # logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n\n\n\n# Main loop: collect experience in env and update/log each epoch\nfor t in range(total_steps):\n    # Until start_steps have elapsed, randomly sample actions\n    # from a uniform distribution for better exploration. Afterwards,\n    # use the learned policy (with some noise, via act_noise).\n    if t &gt; start_steps:\n        action = get_action(o, act_noise)\n    else:\n        action = env.action_space.sample()\n\n    # Step the env\n    obs2, reward, done, _ = env.step(a)\n    ep_ret += reward\n    ep_len += 1\n\n    # Ignore the \"done\" signal if it comes from hitting the time\n    # horizon (that is, when it's an artificial terminal signal\n    # that isn't based on the agent's state)\n    done = False if ep_len == max_ep_len else done\n\n    # Store experience to replay buffer\n    replay_buffer.store(obs, action, reward, obs2, done)\n\n    # Super critical, easy to overlook step: make sure to update\n    # most recent observation!\n    obs = obs2\n\n    # End of trajectory handling\n    if done or (ep_len == max_ep_len):\n        # logger.store(EpRet=ep_ret, EpLen=ep_len)\n        obs, ep_ret, ep_len = env.reset(), 0, 0\n\n    # Update handling\n    if t &gt;= update_after and t % update_every == 0:\n        for _ in range(update_every):\n            batch = replay_buffer.sample_batch(batch_size)\n            feed_dict = {x_ph: batch['obs1'],\n                            x2_ph: batch['obs2'],\n                            a_ph: batch['acts'],\n                            r_ph: batch['rews'],\n                            d_ph: batch['done']\n                        }\n\n            # Q-learning update\n            outs = sess.run([q_loss, q, train_q_op], feed_dict)\n            # logger.store(LossQ=outs[0], QVals=outs[1])\n\n            # Policy update\n            outs = sess.run([pi_loss, train_pi_op, target_update], feed_dict)\n            # logger.store(LossPi=outs[0])\n\n    # End of epoch wrap-up\n    if (t+1) % steps_per_epoch == 0:\n        epoch = (t+1) // steps_per_epoch\n\n        # Save model\n        # if (epoch % save_freq == 0) or (epoch == epochs):\n        #     logger.save_state({'env': env}, None)\n\n        # Test the performance of the deterministic version of the agent.\n        test_agent()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "senior project",
    "section": "",
    "text": "First Attempt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/SquarePacking/Attempt1.html",
    "href": "posts/SquarePacking/Attempt1.html",
    "title": "First Attempt",
    "section": "",
    "text": "from typing import Union, Tuple, List\nfrom PIL import Image, ImageDraw\nimport math\nimport random\nfrom sympy import flatten\nfrom numpy.linalg import norm\n\nimport numpy as np\nfrom typing import List, Tuple\nimport math\nfrom math import cos, sin, tan, pi\nfrom shapely.geometry import MultiPolygon, Polygon, Point\nfrom shapely.affinity import rotate\nfrom shapely.ops import unary_union\n\nimport gymnasium as gym\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n2024-04-30 19:29:07.139573: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-04-30 19:29:07.139892: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-04-30 19:29:07.144709: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-04-30 19:29:07.200270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-04-30 19:29:08.563706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n# Finds the straight-line distance between two points\ndef dist(ax, ay, bx, by):\n    # return norm(np.array((ax, ay))-np.array((bx, by)))\n    return math.sqrt((by - ay)**2 + (bx - ax)**2)\n\n# Rotates point `A` about point `B` by `angle` radians clockwise.\ndef rotated_about(ax, ay, bx, by, angle):\n    radius = dist(ax,ay,bx,by)\n    angle += math.atan2(ay-by, ax-bx)\n    return (\n        round(bx + radius * math.cos(angle)),\n        round(by + radius * math.sin(angle))\n    )\n\n\n\n# Credit for this function goes to ChatGPT\n'''\ndef overlap_area(squares: List[Tuple[float, float, float]]) -&gt; float:\n    # Create a dictionary to keep track of the number of squares overlapping each point\n    overlap_counts = {}\n\n    for rotated_corners in compute_corners(squares):\n        # Add 1 to the overlap count for each point that this square covers\n        for i in range(len(rotated_corners)):\n            p1, p2 = rotated_corners[i], rotated_corners[(i+1)%len(rotated_corners)]\n            for x in range(math.ceil(min(p1[0], p2[0])), math.floor(max(p1[0], p2[0]))):\n                for y in range(math.ceil(min(p1[1], p2[1])), math.floor(max(p1[1], p2[1]))):\n                    if (x, y) in overlap_counts:\n                        overlap_counts[(x, y)] += 1\n                    else:\n                        overlap_counts[(x, y)] = 1\n\n    # Compute the total area of overlap by summing the area of each overlapping square\n    overlap_area = 0\n    for count in overlap_counts.values():\n        if count &gt; 1:\n            overlap_area += 1\n\n    return overlap_area\n '''\n'''\ndef compute_intersection(p1, q1, p2, q2):\n    x1, y1 = p1\n    x2, y2 = q1\n    x3, y3 = p2\n    x4, y4 = q2\n\n    denom = (y4-y3)*(x2-x1) - (x4-x3)*(y2-y1)\n    if denom == 0:\n        return None\n    ua = ((x4-x3)*(y1-y3) - (y4-y3)*(x1-x3)) / denom\n    ub = ((x2-x1)*(y1-y3) - (y2-y1)*(x1-x3)) / denom\n\n    if 0 &lt;= ua &lt;= 1 and 0 &lt;= ub &lt;= 1:\n        return (x1 + ua*(x2-x1), y1 + ua*(y2-y1))\n    else:\n        return None\n\n\ndef overlap_area(squares: List[Tuple[float, float, float]]) -&gt; float:\n    # Create a set to keep track of all the overlapping points\n    overlap_points = set()\n\n    # Iterate over all pairs of squares\n    for i in range(len(squares)):\n        for j in range(i+1, len(squares)):\n            # Compute the corners of each square\n            corners_i = compute_corners([squares[i]])[0]\n            corners_j = compute_corners([squares[j]])[0]\n\n            # Iterate over all pairs of edges\n            for k in range(len(corners_i)):\n                for l in range(len(corners_j)):\n                    # Compute the intersection point of the edges\n                    p = compute_intersection(corners_i[k], corners_i[(k+1)%len(corners_i)], corners_j[l], corners_j[(l+1)%len(corners_j)])\n                    if p is not None:\n                        overlap_points.add(p)\n\n    # Compute the total area of overlap as the sum of the areas of the triangles formed by each set of three adjacent points\n    overlap_area = 0\n    overlap_points = sorted(list(overlap_points))\n    for i in range(1, len(overlap_points)-1):\n        a, b, c = overlap_points[i-1], overlap_points[i], overlap_points[i+1]\n        overlap_area += abs((b[0]-a[0])*(c[1]-a[1]) - (c[0]-a[0])*(b[1]-a[1]))/2\n\n    return overlap_area\n '''\nfrom shapely.geometry import Polygon, MultiPolygon\n'''\ndef get_overlapping_polygons(squares: List[Tuple[float, float, float]]) -&gt; List[Polygon]:\n    polygons = []\n    for square in squares:\n        x, y, rad = square\n        # calculate the coordinates of the four corners of the square\n        p1 = (x + 0.5 * cos(rad - pi / 4), y + 0.5 * sin(rad - pi / 4))\n        p2 = (x + 0.5 * cos(rad + pi / 4), y + 0.5 * sin(rad + pi / 4))\n        p3 = (x + 0.5 * cos(rad + 3 * pi / 4), y + 0.5 * sin(rad + 3 * pi / 4))\n        p4 = (x + 0.5 * cos(rad - 3 * pi / 4), y + 0.5 * sin(rad - 3 * pi / 4))\n        # create a polygon from the four corners\n        polygon = Polygon([p1, p2, p3, p4])\n        # check if this polygon overlaps with any of the previously created polygons\n        overlaps = False\n        for p in polygons:\n            if polygon.intersects(p):\n                overlaps = True\n                # merge the overlapping polygons\n                polygon = polygon.union(p)\n                polygons.remove(p)\n        if not overlaps:\n            # add the polygon to the list\n            polygons.append(polygon)\n    return polygons\n '''\n\ndef convert2shapely(squares:List[Tuple[float, float, float]], side_len:float=1.0) -&gt; List[Polygon]:\n    return [Polygon(corners) for corners in compute_corners(squares, sideLen=side_len)]\n\ndef get_overlapping_polygons(squares: List[Tuple[float, float, float]], side_len: float = 1.0) -&gt; List[Polygon]:\n    overlap = []\n    squares = convert2shapely(squares)\n    for p1 in squares:\n        for p2 in squares:\n            if p1 == p2:\n                continue\n            if p1.intersects(p2):\n                print('Intersection detected')\n                overlap.append(p1.union(p2))\n    return overlap\n\n    # for x, y, rad in squares:\n    #     # compute the corners of the square based on its center and orientation\n    #     corners = compute_corners([square], side_len)[0]\n    #     # create a polygon from the corners\n    #     polygon = Polygon(corners)\n    #     # check if this polygon overlaps with any of the previously created polygons\n    #     overlaps = False\n    #     for p in polygons:\n    #         if polygon.intersects(p):\n    #             overlaps = True\n    #             # merge the overlapping polygons\n    #             polygon = polygon.union(p)def drawBox(draw, x, y, rot, sideLen):\n    square_vertices = (\n        (x + sideLen / 2, y + sideLen / 2),\n        (x + sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y + sideLen / 2)\n    )\n\n    square_vertices = [rotated_about(_x, _y, x, y, rot) for _x, _y in square_vertices]\n    draw.polygon(square_vertices, fill='#FF7B00')\n\n\ndef graphBoxes(squares:List[Tuple['x','y','rad']], squareSize=20, verbose=False):\n    print(f'Size is: {get_bounding_box(compute_corners(squares, squareSize), square=True):.3f}')\n    print(f'Overlapping area is: {overlap_area(squares)}')\n    print(f'Wasted Space is: ')\n\n    # Create image\n    image = Image.new(\"RGB\", (200, 200), \"white\")\n    draw = ImageDraw.Draw(image)\n\n    # Do some calculations\n    corners = compute_corners(squares, squareSize)\n    cornersX, cornersY = np.array(corners).T\n    bottomRight = cornersX.max(), cornersY.max()\n    topLeft = cornersX.min(), cornersY.min()\n\n    # Draw all the boxes\n    for x, y, rad in squares:\n        drawBox(draw, x, y, rad, squareSize)\n\n    if verbose:\n        # Draw the corners\n        for x, y in flatten(corners, levels=1):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#3DAEE9')\n\n        # Draw the bounding box corners\n        for x, y in (topLeft, bottomRight):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#F4C02C')\n\n        # Draw bounding box\n        w, h = get_bounding_box(corners, square=False)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+h)), outline='#383838')\n\n        # Draw bounding square\n        w = get_bounding_box(corners, square=True)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+w)), outline='#8399A4')\n\n        # Draw the overlapping areas\n        for polygon in get_overlapping_polygons(squares, side_len=squareSize):\n            vertices = polygon.exterior.coords\n            # draw the polygon using the polygon method\n            draw.polygon(vertices, fill=(255, 0, 0, 128), outline=(255, 0, 0, 255))\n\n    return image\n\nsqs = convert2shapely([(random.uniform(50, 150), random.uniform(50, 150), random.uniform(1, 2*math.pi)) for i in range(5)])\ngraphBoxes(sqs, verbose=True)\n    #             polygons.remove(p)\n    #     if not overlaps:\n    #         # add the polygon to the list\n    #         polygons.append(polygon)\n    # return polygons\n\nTypeError: cannot unpack non-iterable Polygon object\n\n\n\n# convert2shapely([(random.uniform(50, 150), random.uniform(50, 150), random.uniform(1, 2*math.pi)) for i in range(4)])[0]\n\n\ndef get_bounding_box(corners, square=False):\n    x, y = np.array(corners).T\n    if square:\n        return max(x.max() - x.min(), y.max() - y.min())\n    else:\n        return x.max() - x.min(), y.max() - y.min()\n\n\ndef drawBox(draw, polygon):\n    square_vertices = (\n        (x + sideLen / 2, y + sideLen / 2),\n        (x + sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y + sideLen / 2)\n    )\n\n    square_vertices = [rotated_about(_x, _y, x, y, rot) for _x, _y in square_vertices]\n    draw.polygon(polygon.exterior.coords, fill='#FF7B00')\n\n\ndef graphBoxes(squares:MultiPolygon, squareSize=20, verbose=False):\n    print(f'Size is: {get_bounding_box(compute_corners(squares, squareSize), square=True):.3f}')\n    print(f'Overlapping area is: {overlap_area(squares)}')\n    print(f'Wasted Space is: ')\n\n    # Create image\n    image = Image.new(\"RGB\", (200, 200), \"white\")\n    draw = ImageDraw.Draw(image)\n\n    # Do some calculations\n    corners = compute_corners(squares, squareSize)\n    cornersX, cornersY = np.array(corners).T\n    bottomRight = cornersX.max(), cornersY.max()\n    topLeft = cornersX.min(), cornersY.min()\n\n    # Draw all the boxes\n    for x, y, rad in squares:\n        drawBox(draw, x, y, rad, squareSize)\n\n    if verbose:\n        # Draw the corners\n        for x, y in flatten(corners, levels=1):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#3DAEE9')\n\n        # Draw the bounding box corners\n        for x, y in (topLeft, bottomRight):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#F4C02C')\n\n        # Draw bounding box\n        w, h = get_bounding_box(corners, square=False)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+h)), outline='#383838')\n\n        # Draw bounding square\n        w = get_bounding_box(corners, square=True)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+w)), outline='#8399A4')\n\n        # Draw the overlapping areas\n        for polygon in get_overlapping_polygons(squares, side_len=squareSize):\n            vertices = polygon.exterior.coords\n            # draw the polygon using the polygon method\n            draw.polygon(vertices, fill=(255, 0, 0, 128), outline=(255, 0, 0, 255))\n\n    return image\n\nsqs = convert2shapely([(random.uniform(50, 150), random.uniform(50, 150), random.uniform(1, 2*math.pi)) for i in range(5)], side_len=20)\ngraphBoxes(sqs, verbose=True)\n\nTypeError: cannot unpack non-iterable Polygon object\n\n\n\ndef drawBox(draw, x, y, rot, sideLen):\n    square_vertices = (\n        (x + sideLen / 2, y + sideLen / 2),\n        (x + sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y - sideLen / 2),\n        (x - sideLen / 2, y + sideLen / 2)\n    )\n\n    square_vertices = [rotated_about(_x, _y, x, y, rot) for _x, _y in square_vertices]\n    draw.polygon(square_vertices, fill='#FF7B00')\n\n\ndef graphBoxes(squares:List[Tuple['x','y','rad']], squareSize=20, verbose=False):\n    print(f'Size is: {get_bounding_box(compute_corners(squares, squareSize), square=True):.3f}')\n    print(f'Overlapping area is: {overlap_area(squares)}')\n    print(f'Wasted Space is: ')\n\n    # Create image\n    image = Image.new(\"RGB\", (200, 200), \"white\")\n    draw = ImageDraw.Draw(image)\n\n    # Do some calculations\n    corners = compute_corners(squares, squareSize)\n    cornersX, cornersY = np.array(corners).T\n    bottomRight = cornersX.max(), cornersY.max()\n    topLeft = cornersX.min(), cornersY.min()\n\n    # Draw all the boxes\n    for x, y, rad in squares:\n        drawBox(draw, x, y, rad, squareSize)\n\n    if verbose:\n        # Draw the corners\n        for x, y in flatten(corners, levels=1):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#3DAEE9')\n\n        # Draw the bounding box corners\n        for x, y in (topLeft, bottomRight):\n            draw.ellipse((x-2, y-2, x+2, y+2), fill='#F4C02C')\n\n        # Draw bounding box\n        w, h = get_bounding_box(corners, square=False)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+h)), outline='#383838')\n\n        # Draw bounding square\n        w = get_bounding_box(corners, square=True)\n        draw.rectangle((topLeft, (topLeft[0]+w, topLeft[1]+w)), outline='#8399A4')\n\n        # Draw the overlapping areas\n        for polygon in get_overlapping_polygons(squares, side_len=squareSize):\n            vertices = polygon.exterior.coords\n            # draw the polygon using the polygon method\n            draw.polygon(vertices, fill=(255, 0, 0, 128), outline=(255, 0, 0, 255))\n\n    return image\n\nsqs = [(random.uniform(50, 150), random.uniform(50, 150), random.uniform(1, 2*math.pi)) for i in range(5)]\ngraphBoxes(sqs, verbose=True)\n\nNameError: name 'compute_corners' is not defined\n\n\n\nSetting up the loss function\n\ndef compute_corners(squares: List[Tuple[float, float, float]], sideLen=1):\n    rtn = []\n    for x, y, rot_rad in squares:\n        # Compute the coordinates of the four corners of the square\n        half_side = sideLen / 2\n        corners = [(half_side, half_side), (half_side, -half_side), (-half_side, -half_side), (-half_side, half_side)]\n        rotated_corners = []\n        for corner in corners:\n            rotated_x = x + corner[0]*math.cos(rot_rad) - corner[1]*math.sin(rot_rad)\n            rotated_y = y + corner[0]*math.sin(rot_rad) + corner[1]*math.cos(rot_rad)\n            rotated_corners.append((rotated_x, rotated_y))\n        rtn.append(rotated_corners)\n    return rtn\n\n\ndef overlap_area(multi):\n    overlapArea = 0\n    for i, square1 in enumerate(multi.geoms):\n        for square2 in list(multi.geoms)[i+1:]:\n            if square1.intersects(square2):\n                overlapArea += square1.intersection(square2).area\n    return overlapArea\n\ndef side_len(polygon):\n    x, y = polygon.minimum_rotated_rectangle.exterior.coords.xy\n    edge_length = (Point(x[0], y[0]).distance(Point(x[1], y[1])), Point(x[1], y[1]).distance(Point(x[2], y[2])))\n    return max(edge_length)\n\ndef wasted_space(multi):\n    side = side_len(multi)\n    return side**2 - multi.area\n\n\ndef lossFunc(squares:MultiPolygon):\n    score = 0\n    if not squares.is_valid:\n        # We don't like it when they overlap at all\n        score -= 1000\n        # We don't like it when they overlap a lot\n        score -= math.e**overlap_area(squares)\n\n    score -= wasted_space(squares)\n    score -= side_len(squares)\n    return score\n\n\nN=11\nverbose = True\nscale = 1\nspace = N*scale # Optimal: 3.789, best known: 3.877084\nseed = 42\n\n\n# This is useful. I don't know *how*, but it's got to be somehow\n# squares.convex_hull\nsquares = MultiPolygon(convert2shapely([(random.uniform(0, space), random.uniform(0, space), random.uniform(1, 2*math.pi)) for i in range(N)], side_len=scale))\n\nif verbose:\n    display(unary_union((squares, squares.minimum_rotated_rectangle.exterior)))\nelse:\n    display(squares)\n\nprint(f'Overlaps: {not squares.is_valid}')\nprint(f'Overlap Area: {overlap_area(squares):.3f}')\nprint(f'Side Length: {side_len(squares):.3f}')\nprint(f'Wasted Space: {wasted_space(squares):.2f}')\nprint(f'Loss: {lossFunc(squares)}')\n\nNameError: name 'compute_corners' is not defined\n\n\n\nsquares = MultiPolygon(convert2shapely([(random.uniform(0, space), random.uniform(0, space), random.uniform(1, 2*math.pi)) for i in range(N)], side_len=scale))\nsquares\n\n\n\n\n\n\n\n\n\nsquares\n\n\n\n\n\n\n\n\n\n\nTesting gym\n\n# !pip install gymnasium\nfrom typing import Union, Tuple, List\nfrom PIL import Image, ImageDraw\nimport math\nimport random\nfrom sympy import flatten\nfrom numpy.linalg import norm\n\nimport numpy as np\nfrom typing import List, Tuple\nimport math\nfrom math import cos, sin, tan, pi\nfrom shapely.geometry import MultiPolygon, Polygon, Point\nfrom shapely.affinity import rotate\nfrom shapely.ops import unary_union\n\nimport gymnasium as gym\n\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n%load_ext autoreload\n%autoreload 2\n!pip install -e SquarePacking\n\nDefaulting to user installation because normal site-packages is not writeable\nERROR: SquarePacking is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\n\n\n\n# from SquarePacking.SquarePacking.SquareEnv import SquareEnv\nimport SquarePacking.env\nimport gymnasium as gym\nseed = 42\n\nenv = gym.make(\"SquarePacking/Square-v0\", N=10, render_mode=\"pygame\", shift_rate=.05, rot_rate=.05, bound_method='loop', flatten=True, boundary=1)\ndisplay(env.reset())\n# env = SquareEnv(N=4, search_space=None, shift_rate=.01, rot_rate=.001)\n\nModuleNotFoundError: No module named 'SquarePacking'\n\n\n\nimport time\n\n# action = np.array([(\n#             random.uniform(-env.shift_rate, env.shift_rate),\n#             random.uniform(-env.shift_rate, env.shift_rate),\n#             random.uniform(-env.rot_rate,   env.rot_rate)\n#         ) for _ in range(env.N)])\n\nenv.reset()\naction = np.array([(-env.shift_rate, -env.shift_rate, -env.rot_rate) for _ in range(env.N)]).flatten()\n\nfor _ in range(200):\n    time.sleep(.02)\n    obs, reward, done, _, info = env.step(action)\n    # print(obs[0])\nenv.close()\n\nNameError: name 'env' is not defined\n\n\n\nenv.close()\n\nNameError: name 'env' is not defined\n\n\n\npygame_surface = pygame.image.load(io.BytesIO(env.squares.svg(20, '#d12d2d', 175).encode()))\n\nNameError: name 'pygame' is not defined"
  },
  {
    "objectID": "posts/SquarePacking/Attempt3.html",
    "href": "posts/SquarePacking/Attempt3.html",
    "title": "senior project",
    "section": "",
    "text": "# Install dependancies\n# Don't run this if you installed via requirements.txt\n%pip install gymnasium numpy tensorflow matplotlib pygame shapely pydot graphviz Cope\n\n\n# Imports\n# %load_ext autoreload\n# %autoreload 2\n\nimport gymnasium as gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as ks\nimport random\nimport time\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom SquareEnv import SquareEnv\n\n2024-04-30 19:15:01.423806: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-04-30 19:15:01.424380: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-04-30 19:15:01.431510: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2024-04-30 19:15:01.517087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-04-30 19:15:03.295846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n# Configuration & Enviorment\n\n# I don't need it to be determinisitic\n# seed = 42\n\n# Set up the enviorment\n# After several hours messing with useless nested folders and __init__.py files, I gave up and\n# just did it the way that makes sense instead of the way that appeals to the standard\n# So instead of having useless nested directories, I just import directly from the file. I'm not\n# intending to publish the enviorment by itself.\nenv = SquareEnv(\n    N=11,\n    render_mode=\"pygame\",\n    shift_rate=.05,\n    rot_rate=.03,\n    max_steps=300,\n    bound_method='clip',\n    flatten=True,\n    boundary=2,\n    max_overlap=.1\n)\n\nnum_states = np.product(env.observation_space.shape)\nprint(f\"Size of State Space -&gt; {num_states}\")\nnum_actions = np.product(env.action_space.shape)\nprint(f\"Size of Action Space -&gt; {num_actions}\")\n\nupper_bound = env.action_space.high\nlower_bound = env.action_space.low\n\nprint(f\"Max Value of an Action -&gt; {upper_bound}\")\nprint(f\"Min Value of an Action -&gt; {lower_bound}\")\n\nSize of State Space -&gt; 33\nSize of Action Space -&gt; 33\nMax Value of an Action -&gt; [0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05\n 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05\n 0.05 0.03 0.05 0.05 0.03]\nMin Value of an Action -&gt; [-0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03\n -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03\n -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03]\n\n\n\n\n\nimage.png\n\n\n\nclass Buffer:\n    def __init__(self, buffer_capacity=100000, batch_size=64):\n        # Number of \"experiences\" to store at max\n        self.buffer_capacity = buffer_capacity\n        # Num of tuples to train on.\n        self.batch_size = batch_size\n\n        # Its tells us num of times record() was called.\n        self.buffer_counter = 0\n\n        # Instead of list of tuples as the exp.replay concept go\n        # We use different np.arrays for each tuple element\n        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n\n    # Takes (s,a,r,s') obervation tuple as input\n    def record(self, state, action, reward, next_state):\n        # Set index to zero if buffer_capacity is exceeded, replacing old records\n        index = self.buffer_counter % self.buffer_capacity\n\n        self.state_buffer[index] = state\n        self.action_buffer[index] = action\n        self.reward_buffer[index] = reward\n        self.next_state_buffer[index] = next_state\n\n        self.buffer_counter += 1\n\n    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n    # TensorFlow to build a static graph out of the logic and computations in our function.\n    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n    @tf.function\n    def update(self, state_batch, action_batch, reward_batch, next_state_batch):\n        print('update called')\n        # Training and updating Actor & Critic networks.\n        with tf.GradientTape() as tape:\n            # What we think we should do\n            target_actions = target_actor(next_state_batch, training=True)\n            # How good we think that is (the target Q-Network)\n            target_evaluation = target_critic([next_state_batch, target_actions], training=True)\n            y = reward_batch + gamma * target_evaluation\n            critic_value = critic_model([state_batch, action_batch], training=True)\n            # Get the average amount we were off by in predicting how well we would do, squared, and that's the loss\n            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n\n        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n        # print('critic gradients:')\n        # print(critic_grad)\n        critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n\n        with tf.GradientTape() as tape:\n            actions = actor_model(state_batch, training=True)\n            critic_value = critic_model([state_batch, actions], training=True)\n            # Used `-value` as we want to maximize the value given by the critic for our actions\n            actor_loss = -tf.math.reduce_mean(critic_value)\n\n        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n        actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n\n    # We compute the loss and update parameters\n    def learn(self):\n        # Get sampling range\n        record_range = min(self.buffer_counter, self.buffer_capacity)\n        # Randomly sample indices\n        batch_indices = np.random.choice(record_range, self.batch_size)\n\n        # Convert to tensors\n        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n\n        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n\n# This updates target parameters slowly\n# Based on rate `tau`, which is much less than one.\n@tf.function\ndef update_target(target_weights, weights, tau):\n    for (a, b) in zip(target_weights, weights):\n        a.assign(b * tau + a * (1 - tau))\n\n\n# Define Actor & Critic Models\n# Tells a later cell to reset the weights (since we might have changed how we initialize them in this cell)\nINIT_WEIGHTS = False\n\ndef get_actor():\n    # Initialize weights\n    # last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n\n    inputs = layers.Input(shape=(num_states,))\n    out = layers.Dense(256, activation=\"leaky_relu\")(inputs)\n    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n    # outputs = layers.Dense(num_actions, activation='softsign', kernel_initializer=last_init)(out)\n    outputs = layers.Dense(num_actions, activation='softsign')(out)\n\n    # This is rescale the output from between -1 - 1 to the appropriate action space scale\n    outputs = outputs * upper_bound\n    model = tf.keras.Model(inputs, outputs)\n    # model.summary()\n    return model\n\n# Exactly the same, but using Sequential, which I understand better\ndef get_actor_seq():\n    # Initialize weights\n    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n    last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n\n    model = tf.keras.Sequential([\n        layers.Dense(64, activation=\"leaky_relu\", input_shape=(num_states,)),\n        layers.Dense(128, activation=\"leaky_relu\"),\n        layers.Dense(256, activation=\"leaky_relu\"),\n        layers.Dense(64, activation=\"leaky_relu\"),\n        layers.Dense(num_actions, kernel_initializer=last_init),\n        # layers.Dense(num_actions, activation='softmax'),\n        # This is rescale the output from between -1 - 1 to the appropriate action space scale\n        layers.Lambda(lambda x: x * upper_bound)\n    ])\n\n    # model.summary()\n    return model\n\n\ndef get_critic():\n    # State as input\n    state_input = layers.Input(shape=(num_states,))\n    state_out = layers.Dense(16, activation=\"leaky_relu\")(state_input)\n    state_out = layers.Dense(32, activation=\"leaky_relu\")(state_out)\n\n    # Action as input\n    action_input = layers.Input(shape=(num_actions,))\n    action_out = layers.Dense(32, activation=\"leaky_relu\")(action_input)\n\n    # Both are passed through seperate layer before concatenating\n    concat = layers.Concatenate()([state_out, action_out])\n\n    out = layers.Dense(256, activation=\"leaky_relu\")(concat)\n    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n    outputs = layers.Dense(1)(out)\n\n    # Outputs single value for give state-action\n    model = tf.keras.Model([state_input, action_input], outputs)\n    # model.summary()\n    return model\n\n\n# Config\nactor_weights_file = 'actor_weights'\ncritic_weights_file = 'critic_weights'\n\ntarget_actor_weights_file = 'target_actor_weights'\ntarget_critic_weights_file = 'target_critic_weights'\n\nload_weights = False\nloat_target_weights = False\n\nactor_model = get_actor()\ncritic_model = get_critic()\n\ntarget_actor = get_actor()\ntarget_critic = get_critic()\n\nif load_weights:\n    actor_model.load_weights(actor_weights_file)\n    critic_model.load_weights(critic_weights_file)\n\nif loat_target_weights:\n    target_actor.load_weights(target_actor_weights_file)\n    target_critic.load_weights(target_critic_weights_file)\nelse:\n    # Making the weights equal initially\n    target_actor.set_weights(actor_model.get_weights())\n    target_critic.set_weights(critic_model.get_weights())\n\n# Learning rate for actor-critic models\ncritic_lr = 0.002\nactor_lr = 0.0015\n\ncritic_optimizer = tf.keras.optimizers.Adam(critic_lr)\nactor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n\nbuffer = Buffer(50000, 64)\n\n# How many resets we're running for.\ntotal_episodes = 100\n# Discount factor for future rewards\npatience = gamma = 0.99\n# Used to update target networks\ntau = 0.005\n\nstd_dev = 0.2\n# This used to be a custom OUActionNoise class, but it a paper came out saying that\n# gaussian noise works just as well\nnoise = lambda std=std_dev: np.random.normal(scale=std, size=env.action_space.shape)\nadd_noise = True\nnoise_cooldown = actor_lr * 4\n\nreset_weights = True\n\nif not INIT_WEIGHTS:\n    actor_model.save_weights('actor_model_init.weights.h5')\n    critic_model.save_weights('critic_model_init.weights.h5')\n    target_actor.save_weights('target_actor_init.weights.h5')\n    target_critic.save_weights('target_critic_init.weights.h5')\n    INIT_WEIGHTS = True\n\n\n# ks.utils.plot_model(actor_model,   show_layer_activations=True, show_shapes=True)\nks.utils.plot_model(critic_model,  show_layer_activations=True, show_shapes=True)\n# ks.utils.plot_model(target_actor,  show_layer_activations=True, show_shapes=True)\n# ks.utils.plot_model(target_critic, show_layer_activations=True, show_shapes=True)\n\n\n\n\n\n\n\n\n\n# Main Loop\nimport time\nif reset_weights:\n    actor_model.load_weights('actor_model_init.weights.h5')\n    critic_model.load_weights('critic_model_init.weights.h5')\n    target_actor.load_weights('target_actor_init.weights.h5')\n    target_critic.load_weights('target_critic_init.weights.h5')\n\n# To store reward history of each episode (for plotting)\nep_reward_list = []\n# To store average reward history of last few episodes (for plotting)\navg_reward_list = []\n\ntry:\n    # Run through `total_episodes` number of enviorment resets\n    for ep in range(total_episodes):\n        prev_state, _ = env.reset()\n        # The env.search_space here (and after step()) is to normalize the values to within 0-1 so the NN can interpret them\n        # Note that this assumes the search_space is greater than pi/2 (which shouldn't be a problem)\n        prev_state = np.array([prev_state])/env.search_space\n        episodic_reward = 0\n        cnt = 0\n\n        # Run/step through a single episodes\n        while True:\n            cnt += 1\n            env.render()\n            # Slow it down so I can see it doing things\n            # time.sleep(.1)\n            # Show the enviorment (comment this out to run headless)\n            env.print(f'State: {prev_state}')\n\n            # This is the policy -- deciding what action to take\n            # Get the main actor output (i.e. \"which action do I think we should take?\")\n            sampled_actions = tf.squeeze(actor_model(prev_state)).numpy()\n            env.print(f'Action: {sampled_actions}')\n            if add_noise:\n                # This should make the noise fade out over time (proportional to the actor learning rate)\n                # We want to fade the noise over time, *and* as we step through specific episodes\n                total_cooldown   = max(std_dev - (ep  * noise_cooldown), 0)\n                episode_cooldown = max(std_dev - (cnt * noise_cooldown), 0)\n                sampled_actions += noise((total_cooldown + episode_cooldown) / 2)\n                # sampled_actions += noise(max(std_dev - (cnt * noise_cooldown), 0))\n                env.print(f'Action + noise: {sampled_actions}')\n\n            # Make sure action is the action space\n            action = np.clip(sampled_actions, lower_bound, upper_bound)\n\n            # Recieve state and reward from environment.\n            state, reward, done, _, info = env.step(action)\n            state = np.array([state])/env.search_space\n            env.print(f'Fresh State: {state}')\n\n            buffer.record(prev_state, action, reward, state)\n            episodic_reward += reward\n\n            # This is where the Bellman equation is implemented\n            buffer.learn()\n            time.sleep(20)\n            update_target(target_actor.variables, actor_model.variables, tau)\n            update_target(target_critic.variables, critic_model.variables, tau)\n\n            if done:\n                break\n\n            prev_state = state\n\n        # Episode is done, now do some calculations\n        # These are all just for plotting, not actually important to the algorithm\n        ep_reward_list.append(episodic_reward)\n        # Mean of last 40 episodes\n        avg_reward = np.mean(ep_reward_list[-40:])\n        avg_reward_list.append(avg_reward)\n        print(\"Episode * {} * Avg Reward is ==&gt; {}\".format(ep, avg_reward))\n\nfinally:\n    env.close()\n\nAttributeError: in user code:\n\n    File \"/tmp/ipykernel_99968/3958703911.py\", line 82, in update_target  *\n        a.assign(b * tau + a * (1 - tau))\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'assign'\n\n\n\n# Plotting Episodes versus Avg. Rewards graph\nplt.plot(avg_reward_list)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Avg. Epsiodic Reward\")\nplt.show()\n\n\n# Save the weights\nactor_model.save_weights(actor_weights_file)\ncritic_model.save_weights(critic_weights_file)\n\ntarget_actor.save_weights(target_actor_weights_file)\ntarget_critic.save_weights(target_critic_weights_file)\n\nValueError: The filename must end in `.weights.h5`. Received: filepath=actor_weights"
  }
]